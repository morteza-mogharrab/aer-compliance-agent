# AER Compliance Agent ğŸ¤–

**Autonomous AI Agent for Industrial Compliance Auditing**

![Autonomous AI Agent for Industrial Compliance Auditing](industrial-rag-system.jpg)

> From passive RAG to active agent: An autonomous AI system that performs complete compliance audits by orchestrating 7+ tools to achieve goals without manual intervention.

## ğŸ¯ Overview

This project transforms a traditional RAG (Retrieval-Augmented Generation) system into an **autonomous AI agent** that doesn't just answer questionsâ€”it performs work. When you tell it to "audit a facility," it autonomously:

1. Queries regulatory requirements (RAG)
2. Fetches equipment data (Database)
3. Analyzes compliance status
4. Sends email reports
5. Schedules follow-up tasks
6. Logs maintenance actions

**The difference?** The LLM decides which tools to use and in what order. No hardcoded logic. Just goal-driven execution.

## âœ¨ Key Features

- **ğŸ¤– Autonomous Execution**: 100% automated compliance audits with zero manual steps
- **ğŸ”§ Multi-Tool Orchestration**: 7 tools coordinated by LangChain agent framework
- **ğŸ“š RAG Integration**: ChromaDB vector store with 2,653 indexed directive chunks
- **ğŸ“Š Production UI**: Modern Gradio interface with WCAG 2.1 AA compliance
- **âš¡ Real-Time Actions**: Sends emails, schedules tasks, creates logs automatically
- **ğŸ¯ Mock Enterprise Systems**: Simulated DB, Email, and Calendar APIs for immediate deployment

## ğŸ“Š Results

| Metric | Manual Process | With Agent | Improvement |
|--------|---------------|------------|-------------|
| **Audit Time** | 2+ hours | <30 seconds | **240x faster** |
| **Manual Steps** | 10-15 | 0 | **100% automation** |
| **Consistency** | Varies | Deterministic | **100% consistent** |
| **Actions** | None | Email + Schedule + Log | **Full workflow** |

## ğŸ—ï¸ Architecture

```
User Goal â†’ LLM Agent Brain â†’ Tool Selection â†’ Action Execution â†’ Report
              (GPT-4o)         (7 Tools)        (Mock APIs)
```

### 7 Agent Tools

1. **`search_aer_directives`** - RAG query on ChromaDB vector store
2. **`get_facility_equipment`** - Fetch equipment from mock database
3. **`check_calibration_compliance`** - Analyze compliance status
4. **`send_compliance_report`** - Email automation
5. **`schedule_follow_up`** - Calendar scheduling
6. **`log_maintenance_action`** - Maintenance logging
7. **`list_facilities`** - View all facilities

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- OpenAI API key
- ~500MB disk space

### Installation

```bash
# Clone repository
git clone https://github.com/morteza-mogharrab/aer-compliance-agent.git
cd aer-compliance-agent

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Set OpenAI API key
export OPENAI_API_KEY='your-openai-api-key-here'

# Run automated setup
chmod +x setup_agent.sh
./setup_agent.sh
```

The setup script will:
- âœ… Install all dependencies
- âœ… Download AER directive PDFs (if needed)
- âœ… Build ChromaDB vector index
- âœ… Verify agent system
- âœ… Run tests

### Running the Agent

**Option 1: Web Interface (Recommended)**
```bash
python3 agent_app.py
```
Open browser to `http://localhost:7860`

**Option 2: Command Line**
```bash
python3 agent_core.py
```
Interactive mode - type instructions and see the agent work

**Option 3: Demo Scenarios**
```bash
python3 demo_scenarios.py
```
Pre-configured professional demos

**Option 4: Run Tests**
```bash
python3 test_agent.py
```
Validates all components

## ğŸ’¡ Example Usage

### Web Interface

```
User: "Audit facility FAC-AB-001 for Directive 017 compliance and email results to safety@petolab.com"

Agent: [Autonomous execution]
  âœ“ Queried Directive 017 requirements
  âœ“ Fetched 4 equipment items
  âœ“ Found 2 non-compliant (400 and 380 days overdue)
  âœ“ Sent compliance report via email
  âœ“ Scheduled follow-up for 2026-01-25
  âœ“ Logged maintenance actions (2 items)

Result: Complete audit in 28 seconds
```

### Command Line

```bash
python3 agent_core.py "List all facilities"
# Output: Shows FAC-AB-001 and FAC-AB-002

python3 agent_core.py "Audit facility FAC-AB-001"
# Output: Performs complete audit workflow
```

## ğŸ—‚ï¸ Project Structure

```
aer-compliance-agent/
â”œâ”€â”€ agent_core.py              # LangChain agent orchestration
â”œâ”€â”€ agent_tools.py             # Tool definitions (7 tools)
â”œâ”€â”€ agent_app.py               # Gradio web interface
â”œâ”€â”€ mock_db.py                 # Mock enterprise systems
â”œâ”€â”€ demo_scenarios.py          # Pre-configured demos
â”œâ”€â”€ test_agent.py              # Test suite
â”œâ”€â”€ industrial_rag_system.py   # RAG system (ChromaDB)
â”œâ”€â”€ industrial_app.py          # Legacy RAG interface
â”œâ”€â”€ requirements_agent.txt     # Dependencies
â”œâ”€â”€ setup_agent.sh             # Automated setup
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ .gitignore                 # Git ignore rules
â””â”€â”€ chroma_db/                 # Vector database (created by setup)
```

## ğŸ› ï¸ Technology Stack

**Agent Framework**
- LangChain 0.3.13 (Tool Calling Agent)
- GPT-4o (Function calling)
- Pydantic 2.0 (Type safety)

**RAG System**
- ChromaDB 0.4.22 (Vector store)
- OpenAI text-embedding-3-small (Embeddings)
- 2,653 indexed chunks from AER directives

**Interface**
- Gradio 6.0 (Production UI)
- Custom CSS design system
- WCAG 2.1 AA compliant

**Backend**
- Python 3.9+
- Mock enterprise APIs (DB, Email, Calendar)
- pdfplumber (Document processing)

## ğŸ“ How It Works

### The Agentic Difference

**Traditional RAG:**
```
User: "What are calibration requirements?"
System: [Vector search] â†’ [LLM generates answer]
```

**This Agent:**
```
User: "Audit facility FAC-AB-001"
Agent: [Decides autonomously]
  1. What are requirements? â†’ search_aer_directives()
  2. What equipment exists? â†’ get_facility_equipment()
  3. Are they compliant? â†’ check_calibration_compliance()
  4. Found violations! â†’ send_compliance_report()
  5. Schedule fix â†’ schedule_follow_up()
  6. Log it â†’ log_maintenance_action()
```

The LLM makes decisions. Not hardcoded logic.

### Tool Calling Example

```python
from langchain.tools import tool
from pydantic import BaseModel, Field

class FacilityInput(BaseModel):
    facility_id: str = Field(description="Facility ID to audit")

@tool
def check_calibration_compliance(facility_id: str) -> str:
    """
    Checks equipment calibration dates against 1-year requirement.
    Returns detailed list of non-compliant items.
    """
    # Agent reads this docstring to decide when to use this tool
    equipment = mock_db.fetch_equipment(facility_id)
    # ... compliance logic ...
    return report
```

## ğŸ“– Mock Data

The system includes realistic test data for immediate deployment:

### Facilities
- **FAC-AB-001**: Edmonton South Terminal (4 equipment items)
- **FAC-AB-002**: Calgary Processing Plant (1 equipment item)

### Equipment at FAC-AB-001
- `EQ-PUMP-01` - Glycol Pump âŒ **400 days overdue**
- `EQ-METER-04` - Gas Flow Meter âœ… Compliant (120 days)
- `EQ-FLARE-02` - Flare Stack âœ… Compliant (20 days)
- `EQ-METER-05` - Differential Pressure Meter âŒ **380 days overdue**

This creates realistic scenarios where the agent finds actual violations.

## ğŸ”§ Customization

### Add Your Own Tool

```python
# In agent_tools.py

@tool
def your_custom_tool(param: str) -> str:
    """Description the LLM reads to decide when to use this"""
    # Your logic here
    return result

# Add to tools list
audit_tools.append(your_custom_tool)
```

### Replace Mock APIs with Real APIs

```python
# In mock_db.py

def mock_api_send_email(to, subject, body):
    # Replace this mock:
    return {"status": "sent"}
    
    # With real API:
    import requests
    response = requests.post("https://api.sendgrid.com/...", ...)
    return response.json()
```

## ğŸ§ª Testing

Run the complete test suite:

```bash
python3 test_agent.py
```

**Expected output:**
```
âœ… PASS: Module Imports
âœ… PASS: Mock Database
âœ… PASS: Agent Tools
âœ… PASS: Agent Initialization
âœ… PASS: Tool Execution
âœ… PASS: RAG System
âœ… PASS: Full Workflow

Result: 7/7 tests passed
Status: âœ… ALL TESTS PASSED
```

## ğŸ› Troubleshooting

### "OPENAI_API_KEY not set"
```bash
export OPENAI_API_KEY='sk-...'
```

### "Collection not found"
```bash
python3 industrial_rag_system.py  # Rebuilds RAG index
```

### "Module not found"
```bash
pip install -r requirements_agent.txt
```

### Agent not using tools
- Ensure instructions are specific: "Audit FAC-AB-001" > "Tell me about compliance"
- Check verbose mode is enabled in `agent_core.py`

## ğŸ“š Documentation

- **[Installation Guide](INSTALLATION.md)** - Detailed setup instructions
- **[API Documentation](docs/API.md)** - Tool specifications
- **[Architecture Guide](docs/ARCHITECTURE.md)** - System design
- **[Contributing](CONTRIBUTING.md)** - How to contribute

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Alberta Energy Regulator for directive documentation
- LangChain team for the agent framework
- OpenAI for GPT-4o and embedding models
- Gradio team for the UI framework

## ğŸ“§ Contact

**Morteza Mogharrab**
- Email: morteza.mgb@gmail.com
- LinkedIn: [linkedin.com/in/morteza-mogharrab](https://linkedin.com/in/morteza-mogharrab)
- Portfolio: [morteza-mogharrab.com](https://morteza-mogharrab.github.io/)

## ğŸ”— Related Projects

- [Industrial RAG System](https://github.com/morteza-mogharrab/industrial-rag-system) - The original RAG implementation
- [LangChain Documentation](https://python.langchain.com/) - Agent framework
- [ChromaDB](https://www.trychroma.com/) - Vector database

---

**â­ If you find this project helpful, please consider giving it a star!**

Built with â¤ï¸ for demonstrating production-ready AI engineering
